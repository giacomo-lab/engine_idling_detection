{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is a title\n",
    "here is some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "#%%capture \n",
    "#hide output of this cell\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# Qua fatti una seconda var con il tuo path, cosi a seconda di chi lo sta usando mettiamo un hastag allinizio\n",
    "tfrecord_path = 'C:/Users/matthias/Documents/projects/audioset_v1_embeddings/bal_train/--.tfrecord'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, inspect the raw TFR file structure to get an idea of how its built and it looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"end_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 40.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"labels\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 399\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"start_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 30.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"video_id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"--cB2ZVjpnA\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "features {\n",
      "  feature {\n",
      "    key: \"end_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 40.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"labels\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 0\n",
      "        value: 451\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"start_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 30.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"video_id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"--PJHxphWEs\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "features {\n",
      "  feature {\n",
      "    key: \"end_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 40.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"labels\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 27\n",
      "        value: 466\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"start_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 30.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"video_id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"--ekDLDTUXA\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "features {\n",
      "  feature {\n",
      "    key: \"end_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 10.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"labels\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 0\n",
      "        value: 95\n",
      "        value: 137\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"start_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 0.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"video_id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"--aE2O5G5WE\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "features {\n",
      "  feature {\n",
      "    key: \"end_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 210.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"labels\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 427\n",
      "        value: 431\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"start_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 200.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"video_id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"--aaILOrkII\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "features {\n",
      "  feature {\n",
      "    key: \"end_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 60.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"labels\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 375\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"start_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 50.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"video_id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"--ZhevVpy1s\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "features {\n",
      "  feature {\n",
      "    key: \"end_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 40.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"labels\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 32\n",
      "        value: 34\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"start_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 30.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"video_id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"--aO5cdqSAg\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "# Iterate through the dataset and print the raw example data\n",
    "for record in dataset:\n",
    "    example = tf.train.Example.FromString(record.numpy())\n",
    "    print(example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate trough all files of the google AudioSet dataset. Using `filter_for_thunder()` we isolate all entries that have the thunder label and create a new file with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TFRecord writer for the new file\n",
    "def filter_for_thunder(old_file_path, new_file_path, value_to_check, entries_list):\n",
    "    \"\"\"Parse trough a tfrecord file and search for entries related to one specific label, for example thunder (287),\n",
    "    when found, append the whole entrie to a new file.\n",
    "\n",
    "    Args:\n",
    "        old_file_path (str): path of the original TFRecord file\n",
    "        new_file_path (str): path to the newly created file containing only the selected entries\n",
    "        value_to_check (int): value of the label to use as filter \n",
    "        entries_list (empty list): an empty list to append matching entries to \n",
    "    \"\"\"    \n",
    "    with tf.io.TFRecordWriter(new_file_path) as writer: \n",
    "        \n",
    "        for raw_record in tf.data.TFRecordDataset(old_file_path):\n",
    "            example = tf.train.Example()\n",
    "            example.ParseFromString(raw_record.numpy())\n",
    "\n",
    "            if 'labels' in example.features.feature:\n",
    "                label_values = example.features.feature['labels'].int64_list.value\n",
    "                if value_to_check in label_values:\n",
    "                    # If the value is found, write the example to the new TFRecord file\n",
    "                    writer.write(raw_record.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: IMPLEMENT THIS IN THE CODE ABOVE, as it is not possible to append stuff to TFR files, we need to append \n",
    "# all mathing entries to an empty list and write the file once the loop is done \n",
    "\n",
    "\n",
    "\n",
    "# Define the path to the original TFRecord file and the new TFRecord file\n",
    "original_filename = 'original.tfrecord'\n",
    "new_filename = 'new_filtered.tfrecord'\n",
    "\n",
    "# Define the value to check for (e.g., 287)\n",
    "value_to_check = 287\n",
    "\n",
    "# Create a list to store the matching entries\n",
    "matching_entries = []\n",
    "\n",
    "# Read the original TFRecord file and append the matching entries to the list\n",
    "for raw_record in tf.data.TFRecordDataset(original_filename):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "\n",
    "    if 'labels' in example.features.feature:\n",
    "        label_values = example.features.feature['labels'].int64_list.value\n",
    "        if value_to_check in label_values:\n",
    "            matching_entries.append(raw_record.numpy())\n",
    "\n",
    "# Write the updated list to the new TFRecord file\n",
    "with tf.io.TFRecordWriter(new_filename) as writer:\n",
    "    for entry in matching_entries:\n",
    "        writer.write(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not a valid compression_type: \"a\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matthias\\Documents\\Projects\\thunder_recognition\\thunder_nb.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Define the value to check for (e.g., 287)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m value_to_check \u001b[39m=\u001b[39m \u001b[39m95\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m filter_for_thunder(tfrecord_path, new_filename, value_to_check)\n",
      "\u001b[1;32mc:\\Users\\matthias\\Documents\\Projects\\thunder_recognition\\thunder_nb.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Parse trough a tfrecord file and search for entries related to one specific label, for example thunder (287),\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwhen found, append the whole entrie to a new file.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    value_to_check (int): value of the label to use as filter \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Create a TFRecord writer for the existing file in append mode ('a')\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mTFRecordWriter(new_file_path, options\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m writer: \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mfor\u001b[39;00m raw_record \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTFRecordDataset(old_file_path):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         example \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mExample()\n",
      "File \u001b[1;32mc:\\Users\\matthias\\miniconda3\\envs\\thunder2\\lib\\site-packages\\tensorflow\\python\\lib\\io\\tf_record.py:291\u001b[0m, in \u001b[0;36mTFRecordWriter.__init__\u001b[1;34m(self, path, options)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Opens file `path` and creates a `TFRecordWriter` writing to it.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39m  ValueError: If valid compression_type can't be determined from `options`.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(options, TFRecordOptions):\n\u001b[1;32m--> 291\u001b[0m   options \u001b[39m=\u001b[39m TFRecordOptions(compression_type\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    293\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39msuper\u001b[39m(TFRecordWriter, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    295\u001b[0m     compat\u001b[39m.\u001b[39mas_bytes(path), options\u001b[39m.\u001b[39m_as_record_writer_options())\n",
      "File \u001b[1;32mc:\\Users\\matthias\\miniconda3\\envs\\thunder2\\lib\\site-packages\\tensorflow\\python\\lib\\io\\tf_record.py:86\u001b[0m, in \u001b[0;36mTFRecordOptions.__init__\u001b[1;34m(self, compression_type, flush_mode, input_buffer_size, output_buffer_size, window_bits, compression_level, compression_method, mem_level, compression_strategy)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a `TFRecordOptions` instance.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[39mOptions only effect TFRecordWriter when compression_type is not `None`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m  ValueError: If compression_type is invalid.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39m# Check compression_type is valid, but for backwards compatibility don't\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m# immediately convert to a string.\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_compression_type_string(compression_type)\n\u001b[0;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression_type \u001b[39m=\u001b[39m compression_type\n\u001b[0;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflush_mode \u001b[39m=\u001b[39m flush_mode\n",
      "File \u001b[1;32mc:\\Users\\matthias\\miniconda3\\envs\\thunder2\\lib\\site-packages\\tensorflow\\python\\lib\\io\\tf_record.py:121\u001b[0m, in \u001b[0;36mTFRecordOptions.get_compression_type_string\u001b[1;34m(cls, options)\u001b[0m\n\u001b[0;32m    119\u001b[0m   \u001b[39mreturn\u001b[39;00m options\n\u001b[0;32m    120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mNot a valid compression_type: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(options))\n",
      "\u001b[1;31mValueError\u001b[0m: Not a valid compression_type: \"a\""
     ]
    }
   ],
   "source": [
    "#Use the function on a single file to check functionality, can probably be deleted later on\n",
    "new_filename = 'thunder_tryout.tfrecord'\n",
    "# Define the value to check for (e.g., 287)\n",
    "value_to_check = 95\n",
    "\n",
    "filter_for_thunder(tfrecord_path, new_filename, value_to_check)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"end_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 10.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"labels\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 0\n",
      "        value: 95\n",
      "        value: 137\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"start_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 0.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"video_id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"--aE2O5G5WE\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "# check the newly created file if it containes the entries we expect\n",
    "dataset2 = tf.data.TFRecordDataset(new_filename)\n",
    "\n",
    "# Iterate through the dataset and print the raw example data\n",
    "for record in dataset2:\n",
    "    example = tf.train.Example.FromString(record.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk code, keep for now, delete later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THUNDEEEEERRRRR\n",
      "features {\n",
      "  feature {\n",
      "    key: \"end_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 40.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"labels\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 399\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"start_time_seconds\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 30.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"video_id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"--cB2ZVjpnA\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" print('INFO: \\n', info) \""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nuovo approcio da stack overflow: https://stackoverflow.com/questions/65007191/how-to-read-decode-tfrecords-file-see-the-images-inside-and-do-augmentation\n",
    "# questo funyiona un pó meglio, si riescono ad accedere le singole entry. Sembra che alla fine file \n",
    "# TFRecord siano dizionari di dizionari di dizionari o qualcosa del genere. \n",
    "dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "\n",
    "val = 1\n",
    "info = {}\n",
    "\n",
    "\n",
    "for i, raw_record in enumerate(dataset): # keeping at a minimal example of 3, remove for actual code\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    \n",
    "    # Questa parte qua fa il check per vedere se trova entries con thunder (287)\n",
    "    if 'labels' in example.features.feature:\n",
    "        label_values = example.features.feature['labels'].int64_list.value\n",
    "        if 399 in label_values:\n",
    "            print('THUNDEEEEERRRRR')\n",
    "            print(example)\n",
    "    \n",
    "    \n",
    "\"\"\"     for k, v in example.features.feature.items():\n",
    "        if k == 'labels':\n",
    "            #print(k, v, 'NEXT')\n",
    "            info[k] = v\n",
    "            \n",
    "            val += 1 \"\"\"\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\"\"\" print('INFO: \\n', info) \"\"\"\n",
    "#for n, m in info.items():\n",
    " #   if 287 in m:\n",
    "  #      print('THUNDEERRRRRR')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type of \"labels\" Feature: <dtype: 'int64'>\n",
      "Data Type of \"labels\" Feature: <dtype: 'int64'>\n",
      "Data Type of \"labels\" Feature: <dtype: 'int64'>\n",
      "Data Type of \"labels\" Feature: <dtype: 'int64'>\n",
      "Data Type of \"labels\" Feature: <dtype: 'int64'>\n",
      "Data Type of \"labels\" Feature: <dtype: 'int64'>\n",
      "Data Type of \"labels\" Feature: <dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "# Qua faccio un check per capire il data type del feature labels, perché piú avanti mi crea problemi\n",
    "# Define a parsing function to extract and decode the data\n",
    "## Boh sto qua semplicemente restituisce il data type specificato su tf.io.VarLenFeature(tf.int64), \n",
    "## e non il data type reale. Anche cambiare la parte 'features/feature/video_id' non ha nessun effetto\n",
    "\n",
    "\n",
    "def parse_function(example_proto):\n",
    "    feature_description = {\n",
    "        'features/feature/video_id': tf.io.VarLenFeature(tf.int64),\n",
    "        # Add other features and their types based on your TFRecord structure\n",
    "    }\n",
    "\n",
    "    # Parse the example\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Get the 'labels' feature\n",
    "    labels = example['features/feature/video_id']\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Create a TFRecordDataset\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "# Map the parsing function to the dataset\n",
    "parsed_data = dataset.map(parse_function)\n",
    "\n",
    "# Print the data type of the 'labels' feature\n",
    "for labels in parsed_data:\n",
    "    dtype = labels.values.dtype\n",
    "    print(f'Data Type of \"labels\" Feature: {dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and explore TFRecord files. \n",
    "Goal is to filter them in order to only keep files containing a certain label, like thunder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: []\n",
      "time: []\n",
      "time: []\n",
      "time: []\n",
      "time: []\n",
      "time: []\n",
      "time: []\n"
     ]
    }
   ],
   "source": [
    "#Questo approcio é quello che mi da chatGTP, che peró per me non funziona, le entry che mi ritorna sono vuote. \n",
    "# Qua poi in teoria volgio leggere i label (e solo quelli) in modo che piú avanti si possano filtrare i file che contengono il label thunder (287,/m/0ngt1)\n",
    "# La lista dei vari label si puó trovare nel file csv che ho caricato\n",
    "\n",
    "# Define a parsing function to extract and decode the data\n",
    "def parse_function(example_proto):\n",
    "    feature_description = {\n",
    "        'features/feature/': tf.io.VarLenFeature(tf.float32),\n",
    "        # Add other features and their types based on your TFRecord structure\n",
    "    }\n",
    "\n",
    "    # Parse the example\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Decode the 'labels' feature\n",
    "    labels = tf.sparse.to_dense(example['features/feature/'])\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Create a TFRecordDataset\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "# Map the parsing function to the dataset and iterate through the records\n",
    "for labels in dataset.map(parse_function):\n",
    "    # Print the 'labels' feature\n",
    "    print(f'time: {labels.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_5_device_/job:localhost/replica:0/task:0/device:CPU:0}} Feature: context/feature/end_time_seconds (data type: float) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matthias\\Documents\\Projects\\thunder_recognition\\thunder_nb.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTFRecordDataset(tfrecord_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Map the parsing function to the dataset and iterate through the records\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mmap(parse_function):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X10sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     video_id \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mstrings\u001b[39m.\u001b[39mdecode(record[\u001b[39m'\u001b[39m\u001b[39mcontext/feature/video_id\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matthias/Documents/Projects/thunder_recognition/thunder_nb.ipynb#X10sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     start_time_seconds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmake_ndarray(record[\u001b[39m'\u001b[39m\u001b[39mcontext/feature/start_time_seconds\u001b[39m\u001b[39m'\u001b[39m])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\matthias\\miniconda3\\envs\\thunder2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    765\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 766\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[0;32m    767\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    768\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matthias\\miniconda3\\envs\\thunder2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[1;32m--> 749\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[0;32m    750\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[0;32m    751\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[0;32m    752\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[0;32m    754\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matthias\\miniconda3\\envs\\thunder2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3014\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3015\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 3016\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   3017\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m   3018\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matthias\\miniconda3\\envs\\thunder2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_5_device_/job:localhost/replica:0/task:0/device:CPU:0}} Feature: context/feature/end_time_seconds (data type: float) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "tfrecord_path = 'C:/Users/matthias/Documents/projects/audioset_v1_embeddings/bal_train/-0.tfrecord'\n",
    "\n",
    "# tfrecord_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "\n",
    "# Define a parsing function to extract and decode the data\n",
    "def parse_function(example_proto):\n",
    "    feature_description = {\n",
    "        'context/feature/video_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'context/feature/start_time_seconds': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'context/feature/end_time_seconds': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'context/feature/labels': tf.io.VarLenFeature(tf.int64),\n",
    "        'feature_lists/feature_list/audio_embedding': tf.io.VarLenFeature(tf.string),\n",
    "        # Define other features and their types based on your TFRecord structure\n",
    "    }\n",
    "    \n",
    "    # Parse the example\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # If 'end_time_seconds' may not be present in all records, use allow_missing=True\n",
    "    if 'context/feature/end_time_seconds' in example:\n",
    "        end_time_seconds = example['context/feature/end_time_seconds']\n",
    "    else:\n",
    "        end_time_seconds = 0.0  # Provide a default value\n",
    "    \n",
    "    \n",
    "    # Decode the VarLenFeatures\n",
    "    example['context/feature/labels'] = tf.sparse.to_dense(example['context/feature/labels'], default_value=0)\n",
    "    example['feature_lists/feature_list/audio_embedding'] = tf.sparse.to_dense(example['feature_lists/feature_list/audio_embedding'], default_value='b')\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Create a TFRecordDataset\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "# Map the parsing function to the dataset and iterate through the records\n",
    "for record in dataset.map(parse_function):\n",
    "    video_id = tf.strings.decode(record['context/feature/video_id'], 'utf-8')\n",
    "    start_time_seconds = tf.make_ndarray(record['context/feature/start_time_seconds'])[0]\n",
    "    end_time_seconds = tf.make_ndarray(record['context/feature/end_time_seconds'])[0]\n",
    "    labels = tf.make_ndarray(record['context/feature/labels'])\n",
    "    audio_embedding = record['feature_lists/feature_list/audio_embedding']\n",
    "    \n",
    "    print(f'Video ID: {video_id}')\n",
    "    print(f'Start Time (seconds): {start_time_seconds}')\n",
    "    print(f'End Time (seconds): {end_time_seconds}')\n",
    "    print(f'Labels: {labels}')\n",
    "    print(f'Audio Embedding: {audio_embedding}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thunder_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
